{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44f71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up (ne sera pas dans la présentation, seulement dans ce notebook):\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "n = 1_000_000\n",
    "\n",
    "n_dicts = 10_000\n",
    "list_of_dicts = [\n",
    "    dict(zip(np.random.choice(10**10, n // n_dicts), range(n)))\n",
    "    for _ in range(n_dicts)\n",
    "]\n",
    "\n",
    "n_users = 100_000\n",
    "df = pd.DataFrame({'user': np.random.choice(n_users, n), 'item': np.random.choice(100, n)})\n",
    "\n",
    "n_groups = 100_000\n",
    "x = np.random.rand(n)\n",
    "group_idx = np.repeat(np.arange(n_groups), n // n_groups)\n",
    "np.random.shuffle(group_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde107c2",
   "metadata": {},
   "source": [
    "(début de la présentation)\n",
    "\n",
    "### Quel est le point commun de ces 3 fonctions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afdc5a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(list_of_dicts: list[dict]) -> dict:\n",
    "    return reduce(lambda d1, d2: {**d1, **d2}, list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d97a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_uniques_by_user(df: pd.DataFrame) -> dict:\n",
    "    n_uniques = {}\n",
    "    for user in df['user'].unique():\n",
    "        user_activity = df[df['user'] == user]\n",
    "        n_uniques[user] = user_activity['item'].nunique()\n",
    "    return n_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a811e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_by_group(group_idx: np.ndarray[int], x: np.ndarray) -> np.ndarray:\n",
    "    n_groups = group_idx.max() + 1\n",
    "    avg_by_group = np.empty(n_groups)\n",
    "    for idx in range(n_groups):\n",
    "        avg_by_group[idx] = np.mean(x[group_idx == idx])\n",
    "    return avg_by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd1e0bf",
   "metadata": {},
   "source": [
    "Indice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d809c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sum(len(d) for d in list_of_dicts)\n",
    "n = len(df)\n",
    "n = len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd0e86c",
   "metadata": {},
   "source": [
    "### Réponse: L'inefficacité algorithmique! \n",
    "\n",
    "En effet, ces 3 petits algos peuvent être en $ O(n^2) $, selon les données.\n",
    "\n",
    "Et pourtant j'ai vu ce pattern un grand nombre de fois:\n",
    "- l'exemple avec les dictionnaires, je l'ai vue tel quel: une collègue m'a appelé parce qu'elle ne comprenait pas pourquoi c'était si lent.\n",
    "- l'exemple avec pandas, c'est celui que j'ai vue le plus! Les data scientists l'emploient souvent lorsque ce qu'ils veulent faire n'est pas \n",
    "  facilement faisable avec des `groupby` et autres (ou pas faisable du tout, ça peut arriver).\n",
    "- l'exemple avec numpy, ... bah je l'ai vue dans [scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/473fef0bf83882efbc3273c2dfb4d82491e9066b/sklearn/ensemble/_gb.py#L257-L266)! Dans un cas où le $O(n^2)$ arrive pour des hyper-paramètres du modèles peu courants, mais qui doivent quand-même être explorés régulièrement dans des grid search ou similaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df3b54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 1000000, 1000000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(d) for d in list_of_dicts), len(df), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d29a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.9 s, sys: 39.4 s, total: 1min 31s\n",
      "Wall time: 1min 31s\n",
      "CPU times: user 1min 21s, sys: 13.5 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n",
      "CPU times: user 1min 4s, sys: 4.82 ms, total: 1min 4s\n",
      "Wall time: 1min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.46392534, 0.5279945 , 0.43661711, ..., 0.46826864, 0.48314751,\n",
       "       0.65158763], shape=(100000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time merge_dicts(list_of_dicts)\n",
    "%time compute_n_uniques_by_user(df)\n",
    "%time compute_avg_by_group(group_idx, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885fa3f",
   "metadata": {},
   "source": [
    "### Pourquoi on écrit ça quand-même?\n",
    "\n",
    "Parce que lorsqu'on l'écrit, la complexité algorithmique est caché par la syntaxe.\n",
    "\n",
    "Quand je demande au gens qui ont écrit ce pattern en pandas quelle est la complexité théorique de leur code d'après eux, ils me répondent souvent $O(n)$, car il ne process qu'une fois chaque ligne du dataframe. Ils passent à coté du fait que `df['user'] == user` process toutes les lignes du dataframe.\n",
    "\n",
    "### Et du coup, comment on évite ça?\n",
    "\n",
    "Déjà, il faut bien comprendre Python et les librairies qu'on utilise (pandas, numpy, ...).\n",
    "\n",
    "Ensuite il faut connaître les patterns qui permettent d'éviter ça:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cffcc",
   "metadata": {},
   "source": [
    "### Amélioration de l'exemple avec les dictionnaires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de05050",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = {}\n",
    "for d in list_of_dicts:\n",
    "    merged_dict.update(d)  # O(len(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8bc76",
   "metadata": {},
   "source": [
    "### Amélioration de l'exemple en pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b731d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_uniques_by_user = df.groupby('user')['item'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c8337",
   "metadata": {},
   "source": [
    "Oui ok, mais si c'est plus compliqué ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba2f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assez lent, mais en O(n) donc pas de mauvaise surprise\n",
    "_ = df.groupby('user')['item'].apply(lambda user_items: user_items.nunique(), include_groups=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d657b68",
   "metadata": {},
   "source": [
    "C'est bien plus lent, car on paye un overhead* pour chaque groupe. Mais c'est aussi vrai pour la boucle qu'on avait avant, et là on évite le $O(n^2)$ qui sera juste impossible à faire tourner dès que $n$ va dans les millions.\n",
    "\n",
    "*création d'un petit dataframe pour chaque groupe, ce qui implique de nombreuses ligne de Python, et une ligne de Python ça coûte cher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e66044",
   "metadata": {},
   "source": [
    "### Amélioration de l'exemple en numpy:\n",
    "\n",
    "Une solution facile, c'est de passer par `pandas` + `groupby`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e73c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_by_group = pd.Series(x).groupby(group_idx).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559d78e",
   "metadata": {},
   "source": [
    "Mais parfois on veut éviter pandas. Par exemple, pandas n'est pas une dépendance de scikit-learn, donc on veut résoudre ça sans pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a1b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_by_group = np.bincount(group_idx, minlength=n_groups)\n",
    "sum_by_group = np.bincount(group_idx, weights=x, minlength=n_groups)\n",
    "\n",
    "avg_by_group = sum_by_group / n_by_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc04e9",
   "metadata": {},
   "source": [
    "Ok, mais bincount ça fait seulement des sommes / des histogrammes, et si on veut faire plus? \n",
    "\n",
    "Par exemple le minimum par groupe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a41375",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_by_group = np.full(n_groups, np.inf)\n",
    "np.minimum.at(min_by_group, group_idx, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580fef7",
   "metadata": {},
   "source": [
    "C'est quoi ce `.at` magique? C'est les méthodes des fonctions universelles de numpy (`ufunc`):\n",
    "https://numpy.org/doc/stable/reference/ufuncs.html#methods\n",
    "\n",
    "\n",
    "De nombreuses fonctions numpy sont des ufuncs, notamment: `add`, `minimum`, `maximum` et même `gcd`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe43c2ee",
   "metadata": {},
   "source": [
    "### Pour aller plus loin:\n",
    "\n",
    "1) Que faire si je n'ai pas des indices denses dans $[0, n_{groups})$ mais des IDs arbitraires?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87aa728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(n)\n",
    "group_id = np.random.choice(np.random.choice(10**18, n_groups), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9872b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids, group_idx = np.unique(group_id, return_inverse=True)\n",
    "# ^ O(n log n), `np.unique` trie son input\n",
    "# En pratique, ce O(n log n) est 10-100x plus lent qu'un `bincount` par exemple,\n",
    "# ce qui reste généralement bien assez rapide. Par exemple, cette cellule\n",
    "# s'exécute en ~70ms pour 1M d'éléments\n",
    "n_groups = unique_ids.size\n",
    "\n",
    "sum_by_group = np.bincount(group_idx, weights=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40246716",
   "metadata": {},
   "source": [
    "2. Que faire si les ufuncs ne suffisent pas? Par exemple, ça ne permet pas de calculer la médiane par groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52fb1b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff413d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_per_group = csr_array((\n",
    "    x,\n",
    "    (group_idx, np.arange(n))\n",
    "), shape=(n_groups, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e305182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_per_row(data, indptr):\n",
    "    n_rows = indptr.size - 1\n",
    "    out = np.full(n_rows, np.nan)\n",
    "    for row in range(n_rows):\n",
    "        row_start, row_end = indptr[row], indptr[row + 1]\n",
    "        if row_start == row_end:\n",
    "            continue\n",
    "        out[row] = np.median(data[row_start:row_end])\n",
    "    return out\n",
    "\n",
    "_ = median_per_row(x_per_group.data, x_per_group.indptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649014e5",
   "metadata": {},
   "source": [
    "Si on a besoin de vitesse, ce genre de fonctions est généralent compatible avec numba:\n",
    "\n",
    "(si on était dans scikit-learn, on ferait plutôt pour une implémentation en Cython)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dea1784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "median_per_row_numba = njit(median_per_row)\n",
    "_ = median_per_row_numba(x_per_group.data, x_per_group.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8840af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921 ms ± 46 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "23.1 ms ± 73.1 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ = median_per_row(x_per_group.data, x_per_group.indptr)\n",
    "%timeit _ = median_per_row_numba(x_per_group.data, x_per_group.indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3a107e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.2 ms ± 2.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ = pd.Series(x).groupby(group_idx).median()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
